{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least square solution(more equations less solutions)\n",
    "# A(N*M) , W(M*1) , Y(N*1)\n",
    "# N>M  (NO EXACT SOLUTION)\n",
    "# Aw=y\n",
    "# w=inv((A.T).A).(A.T).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum norm solution(more parameters less equations)\n",
    "# A(N*M) , W(M*1) , Y(N*1)\n",
    "# N<M (MANY SOLUTION EXISTS)\n",
    "# Aw=y\n",
    "# w=(A.T).inv(A.A.T).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import keras\n",
    "np.random.seed(100)\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYS dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      "\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('KEYS',iris.keys())\n",
    "print()\n",
    "\n",
    "X=iris[\"data\"]\n",
    "Y=iris[\"target\"]\n",
    "\n",
    "print(X[:3])\n",
    "print(Y[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "names=iris[\"target_names\"]\n",
    "features=iris[\"feature_names\"]\n",
    "\n",
    "print(names)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,  24,  67, 103,  87])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isamples = np.random.randint(len(Y) , size=5)\n",
    "isamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4 2.9 1.4 0.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.3 2.3 4.4 1.3]]\n",
      "[0 0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(X[isamples])\n",
    "print(Y[isamples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4 2.9 1.4 0.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.3 2.3 4.4 1.3]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#one-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "ny = len(np.unique(Y))\n",
    "Y = to_categorical(Y[:] , num_classes=ny)\n",
    "\n",
    "print(X[isamples])\n",
    "print(Y[isamples])\n",
    "print()\n",
    "print(ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training-testing split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train , X_test , Y_train , Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_normalization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) #computes mean and std\n",
    "\n",
    "X_train=scaler.transform(X_train) #perform transformation\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to add 1's column\n",
    "\n",
    "addcol = lambda x: np.concatenate((x,np.ones((x.shape[0],1))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx,ny=X_train.shape\n",
    "\n",
    "def findWeights(A,Y):\n",
    "    \n",
    "    w = np.linalg.inv(A.T.dot(A)).dot(A.T.dot(Y))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = addcol(X_train)\n",
    "Y = Y_train\n",
    "w = findWeights(A,Y)\n",
    "yd = A.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y,yd):\n",
    "    print(confusion_matrix(yd,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  0  0]\n",
      " [ 0 22  4]\n",
      " [ 0 15 40]]\n"
     ]
    }
   ],
   "source": [
    "yd = np.argmax(yd,axis=1)\n",
    "y = np.argmax(Y,axis=1)\n",
    "evaluate(y,yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = addcol(X_test)\n",
    "Y = Y_test\n",
    "w = findWeights(A,Y)\n",
    "ynew = A.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  2]\n",
      " [ 0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "yd = np.argmax(ynew,axis=1)\n",
    "y = np.argmax(Y,axis=1)\n",
    "evaluate(y,yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "addSqlcol = lambda x: np.concatenate((x, x**2, np.ones((x.shape[0], 1))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  0  0]\n",
      " [ 0 35  2]\n",
      " [ 0  2 42]]\n"
     ]
    }
   ],
   "source": [
    "A = addSqlcol(X_train)\n",
    "Y = Y_train\n",
    "w = findWeights(A,Y)\n",
    "yd = A.dot(w)\n",
    "yd = np.argmax(yd,axis=1)\n",
    "y = np.argmax(Y,axis=1)\n",
    "evaluate(y,yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "A = addSqlcol(X_test)\n",
    "Y = Y_test\n",
    "w = findWeights(A,Y)\n",
    "yd = A.dot(w)\n",
    "yd = np.argmax(yd,axis=1)\n",
    "y = np.argmax(Y,axis=1)\n",
    "evaluate(y,yd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
